#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

version: '3.8'
services:
  # Reverse proxy routing requests to the Ollama service
  traefik:
    image: traefik:v2.5
    restart: unless-stopped
    command:
      - "--api.insecure=true" # Enables the dashboard and API insecurely
      - "--log.level=DEBUG" # Adjust the log level as needed
      - "--accesslog=true" # Enables access logs
      - "--providers.docker=true"
      - "--providers.docker.exposedByDefault=false"
      - "--entrypoints.web.address=:80"
    ports:
      - "80:80"
      - "8080:8080" # Expose port 8080 for the dashboard
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks:
      - web
    depends_on:
      - ollama
      - auth-service

  # LLM inference service Ollama
  ollama:
    image: ollama/ollama
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ollama.rule=Host(`spezillmfog.local`)"
      - "traefik.http.routers.ollama.entrypoints=web"
      - "traefik.http.routers.ollama.service=ollama-service"
      - "traefik.http.services.ollama-service.loadbalancer.server.port=11434"
      - "traefik.http.routers.ollama.middlewares=auth@docker"
      - "traefik.http.middlewares.auth.forwardauth.address=http://auth-service:3000/"   # Authorizes incoming LLM inference jobs via Firebase Emulator
      - "traefik.http.middlewares.auth.forwardauth.trustForwardHeader=true"     # Forwards all headers to authorization service
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama
    networks:
      - web

  # Authorizes incoming LLM inference requests
  auth-service:
    build:
      context: auth
    hostname: auth-service
    restart: unless-stopped
    environment:
      - PORT=3000
      # Use the Firebase emulator
      - USE_FIREBASE_EMULATOR=true
      - FIREBASE_AUTH_EMULATOR_HOST=firebase-emulator:9099
      - FIREBASE_PROJECT_ID=spezillmfog
    labels:
      - "traefik.enable=false"
    ports:
      - "3000:3000"
    networks:
      - web
    depends_on:
      - firebase-emulator

  # Firebase emulator that authenticates the incoming LLM requests
  firebase-emulator:
    build:
      context: auth/firebaseEmulator
    hostname: firebase-emulator
    restart: unless-stopped
    labels:
      - "traefik.enable=false"
    ports:
      - "4000:4000"   # Expose web UI
      - "9099:9099"   # Expose auth emulator service
    networks:
      - web

# Enables persistence of downloaded LLMs by Ollama
volumes:
  ollama_storage:

networks:
  web:
    driver: bridge
