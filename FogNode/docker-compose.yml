#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

version: '3.8'
services:
  # Reverse proxy authenticating requests and routing them to the Ollama service
  traefik:
    image: traefik:v2.5
    restart: unless-stopped
    command:
      - "--providers.docker=true"
      - "--providers.docker.exposedByDefault=false"
      - "--providers.file.filename=/etc/traefik/certs/dynamic_conf.yml"   # Configures TLS certs
      - "--entrypoints.websecure.address=:443"
    ports:
      - "443:443"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
      - "./certs/webservice:/etc/traefik/certs"   # Mount TLS certs into container
      - "./traefik/dynamic_conf.yml:/etc/traefik/certs/dynamic_conf.yml"  # Mount TLS config into container
    networks:
      - web
    depends_on:
      - ollama
      - auth-service
      - avahi
  
  # LLM inference service Ollama
  ollama:
    image: ollama/ollama
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ollama.rule=Host(`spezillmfog.local`)"
      - "traefik.http.routers.ollama.entrypoints=websecure"
      - "traefik.http.routers.ollama.tls=true"
      - "traefik.http.routers.ollama.service=ollama-service"
      - "traefik.http.services.ollama-service.loadbalancer.server.port=11434"
      - "traefik.http.routers.ollama.middlewares=auth@docker"
      - "traefik.http.middlewares.auth.forwardauth.address=http://auth-service:3000/"   # Authorizes incoming LLM inference jobs via Firebase
      - "traefik.http.middlewares.auth.forwardauth.trustForwardHeader=true"     # Forwards all headers to authorization service
    volumes:
      - ollama_storage:/root/.ollama
    networks:
      - web

  # Authorizes incoming LLM inference requests
  auth-service:
    build:
      context: auth
    hostname: auth-service
    restart: unless-stopped
    environment:
      - PORT=3000
    labels:
      - "traefik.enable=false"
    volumes:
      - ./auth/serviceAccountKey.json:/usr/src/app/serviceAccountKey.json  # Adjust the host mount location as needed
    networks:
      - web

  # On the Linux platform, advertise LLM inference service via mDNS from Avahi
  avahi:
    build:
      context: avahi
    hostname: spezillmfog.local
    network_mode: host  # Need to run in host network mode for mDNS 
    profiles:
      - linux
    restart: unless-stopped

# Enables persistence of downloaded LLMs by Ollama
volumes:
  ollama_storage:

networks:
  web:
    driver: bridge
