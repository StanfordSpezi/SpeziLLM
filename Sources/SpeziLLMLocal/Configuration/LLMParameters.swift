//
// This source file is part of the Stanford Spezi open source project
//
// SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
//
// SPDX-License-Identifier: MIT
//

import Foundation
import llama


/// The ``LLMParameters`` represents the parameters of the LLM.
/// Internally, these data points are passed as a llama.cpp `llama_model_params` C struct to the LLM.
public struct LLMParameters: Sendable {
    /// Typealias for an internal llama.cpp progress callback function
    public typealias LlamaProgressCallback = (@convention(c) (Float, UnsafeMutableRawPointer?) -> Void)
    
    
    /// Indicates the maximum output length generated by the ``LLM``.
    let maxOutputLength: Int
    /// Indicates whether the BOS token is added by the ``LLM``.
    let addBosToken: Bool
    /// Top-K Sampling: K most likely next words.
    let topK: Int32
    /// Top-p Sampling: Smallest possible set of words whose cumulative probability exceeds the probability p.
    let topP: Float
    /// Temperature Sampling: A higher value indicates more creativity of the model but also more hallucinations.
    let temperature: Float
    
    /// Wrapped C struct from the llama.cpp library, later-on passed to the LLM
    private var wrapped: llama_model_params
    
    
    /// Model parameters in llama.cpp's low-level C representation
    var llamaCppRepresentation: llama_model_params {
        wrapped
    }
    
    /// Number of layers to store in VRAM
    /// - Note: On iOS simulators, this property has to be set to 0 (which is automatically done by the library).
    var gpuLayerCount: Int32 {
        get {
            wrapped.n_gpu_layers
        }
        set {
            wrapped.n_gpu_layers = newValue
        }
    }
    
    /// Indicates the GPU that is used for scratch and small tensors.
    var mainGpu: Int32 {
        get {
            wrapped.main_gpu
        }
        set {
            wrapped.main_gpu = newValue
        }
    }
    
    /// Indicates how to split layers across multiple GPUs.
    var tensorSplit: UnsafePointer<Float>? {
        get {
            wrapped.tensor_split
        }
        set {
            wrapped.tensor_split = newValue
        }
    }

    /// Progress callback called with a progress value between 0 and 1
    var progressCallback: LlamaProgressCallback? {
        get {
            wrapped.progress_callback
        }
        set {
            wrapped.progress_callback = newValue
        }
    }
    
    /// Context pointer that is passed to the progress callback
    var progressCallbackUserData: UnsafeMutableRawPointer? {
        get {
            wrapped.progress_callback_user_data
        }
        set {
            wrapped.progress_callback_user_data = newValue
        }
    }

    /// Indicates wether booleans should be kept together to avoid misalignment during copy-by-value.
    var vocabOnly: Bool {
        get {
            wrapped.vocab_only
        }
        set {
            wrapped.vocab_only = newValue
        }
    }
    
    /// Indicates if mmap should be used.
    var useMmap: Bool {
        get {
            wrapped.use_mmap
        }
        set {
            wrapped.use_mmap = newValue
        }
    }
    
    /// Forces the system to keep model in RAM.
    var useMlock: Bool {
        get {
            wrapped.use_mlock
        }
        set {
            wrapped.use_mlock = newValue
        }
    }
    
    
    /// Creates the ``LLMParameters`` which wrap the underlying llama.cpp `llama_model_params` C struct.
    /// Is passed to the underlying llama.cpp model in order to configure the LLM.
    ///
    /// - Parameters:
    ///   - maxOutputLength: The maximum output length generated by the Spezi `LLM`, defaults to `128`.
    ///   - addBosToken: Indicates wether the BOS token is added by the Spezi `LLM`, defaults to `false`.
    ///   - topK: Top-K Sampling: K most likely next words.
    ///   - topP: Top-p Sampling: Smallest possible set of words whose cumulative probability exceeds the probability p.
    ///   - temperature: Temperature Sampling: A higher value indicates more creativity of the model but also more hallucinations.
    ///   - gpuLayerCount: Number of layers to store in VRAM, defaults to `1`, meaning Apple's `Metal` framework is enabled.
    ///   - mainGpu: GPU that is used for scratch and small tensors, defaults to `0` representing the main GPU.
    ///   - tensorSplit: Split layers across multiple GPUs, defaults to `nil`, meaning no split.
    ///   - progressCallback: Progress callback called with a progress value between 0 and 1, defaults to `nil`.
    ///   - progressCallbackUserData: Context pointer that is passed to the progress callback, defaults to `nil`.
    ///   - vocabOnly: Indicates wether booleans should be kept together to avoid misalignment during copy-by-value., defaults to `false`.
    ///   - useMmap: Indicates if mmap should be used., defaults to `true`.
    ///   - useMlock: Forces the system to keep model in RAM, defaults to `false`.
    public init(
        maxOutputLength: Int = 128,
        addBosToken: Bool = false,
        topK: Int32 = 40,
        topP: Float = 0.9,
        temperature: Float = 0.7,
        gpuLayerCount: Int32 = 1,
        mainGpu: Int32 = 0,
        tensorSplit: UnsafePointer<Float>? = nil,
        progressCallback: LlamaProgressCallback? = nil,
        progressCallbackUserData: UnsafeMutableRawPointer? = nil,
        vocabOnly: Bool = false,
        useMmap: Bool = true,
        useMlock: Bool = false
    ) {
        self.wrapped = llama_model_default_params()
        
        self.maxOutputLength = maxOutputLength
        self.addBosToken = addBosToken
        self.topK = topK
        self.topP = topP
        self.temperature = temperature
        
        /// Overwrite `gpuLayerCount` in case of a simulator target environment
        #if targetEnvironment(simulator)
        self.gpuLayerCount = 0     // Disable Metal on simulator as crash otherwise
        #else
        self.gpuLayerCount = gpuLayerCount
        #endif
        self.mainGpu = mainGpu
        self.tensorSplit = tensorSplit
        self.progressCallback = progressCallback
        self.progressCallbackUserData = progressCallbackUserData
        self.vocabOnly = vocabOnly
        self.useMmap = useMmap
        self.useMlock = useMlock
    }
}
