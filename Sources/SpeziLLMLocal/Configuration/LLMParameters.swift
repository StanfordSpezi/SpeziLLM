//
// This source file is part of the Stanford Spezi open source project
//
// SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
//
// SPDX-License-Identifier: MIT
//

import Foundation
import llama


/// The ``LLMParameters`` represents the parameters of the LLM.
/// Internally, these data points are passed as a llama.cpp `llama_model_params` C struct to the LLM.
public struct LLMParameters: Sendable {
    /// Typealias for an internal llama.cpp progress callback function
    public typealias LlamaProgressCallback = (@convention(c) (Float, UnsafeMutableRawPointer?) -> Void)
    
    
    /// Defaults of possible LLMs parameter settings.
    public enum Defaults {
        public static var defaultLlama2SystemPrompt: String {
            // swiftlint:disable line_length
            """
            You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe and still concise. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.
            If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.
            """
            // swiftlint:enable line_length
        }
    }
    
    
    /// The to-be-used system prompt of the LLM
    let systemPrompt: String
    /// Indicates the maximum output length generated by the ``LLM``.
    let maxOutputLength: Int
    /// Indicates whether the BOS token is added by the ``LLM``. If `nil`, the default from the model itself is taken.
    let addBosToken: Bool
    
    
    /// Wrapped C struct from the llama.cpp library, later-on passed to the LLM
    private var wrapped: llama_model_params
    
    
    /// Model parameters in llama.cpp's low-level C representation
    var llamaCppRepresentation: llama_model_params {
        wrapped
    }
    
    /// Number of layers to store in VRAM
    /// - Note: On iOS simulators, this property has to be set to 0 (which is automatically done by the library).
    var gpuLayerCount: Int32 {
        get {
            wrapped.n_gpu_layers
        }
        set {
            wrapped.n_gpu_layers = newValue
        }
    }
    
    /// Indicates the GPU that is used for scratch and small tensors.
    var mainGpu: Int32 {
        get {
            wrapped.main_gpu
        }
        set {
            wrapped.main_gpu = newValue
        }
    }
    
    /// Indicates how to split layers across multiple GPUs.
    var tensorSplit: UnsafePointer<Float>? {
        get {
            wrapped.tensor_split
        }
        set {
            wrapped.tensor_split = newValue
        }
    }

    /// Progress callback called with a progress value between 0 and 1
    var progressCallback: LlamaProgressCallback? {
        get {
            wrapped.progress_callback
        }
        set {
            wrapped.progress_callback = newValue
        }
    }
    
    /// Context pointer that is passed to the progress callback
    var progressCallbackUserData: UnsafeMutableRawPointer? {
        get {
            wrapped.progress_callback_user_data
        }
        set {
            wrapped.progress_callback_user_data = newValue
        }
    }

    /// Indicates wether booleans should be kept together to avoid misalignment during copy-by-value.
    var vocabOnly: Bool {
        get {
            wrapped.vocab_only
        }
        set {
            wrapped.vocab_only = newValue
        }
    }
    
    /// Indicates if mmap should be used.
    var useMmap: Bool {
        get {
            wrapped.use_mmap
        }
        set {
            wrapped.use_mmap = newValue
        }
    }
    
    /// Forces the system to keep model in RAM.
    var useMlock: Bool {
        get {
            wrapped.use_mlock
        }
        set {
            wrapped.use_mlock = newValue
        }
    }
    
    
    /// Creates the ``LLMParameters`` which wrap the underlying llama.cpp `llama_model_params` C struct.
    /// Is passed to the underlying llama.cpp model in order to configure the LLM.
    ///
    /// - Parameters:
    ///   - systemPromot: The to-be-used system prompt of the LLM enabling fine-tuning of the LLMs behaviour. Defaults to the regular Llama2 system prompt.
    ///   - maxOutputLength: The maximum output length generated by the Spezi `LLM`, defaults to `1024`.
    ///   - addBosToken: Indicates wether the BOS token is added by the Spezi `LLM`, defaults to `false`.
    ///   - gpuLayerCount: Number of layers to store in VRAM, defaults to `1`, meaning Apple's `Metal` framework is enabled.
    ///   - mainGpu: GPU that is used for scratch and small tensors, defaults to `0` representing the main GPU.
    ///   - tensorSplit: Split layers across multiple GPUs, defaults to `nil`, meaning no split.
    ///   - progressCallback: Progress callback called with a progress value between 0 and 1, defaults to `nil`.
    ///   - progressCallbackUserData: Context pointer that is passed to the progress callback, defaults to `nil`.
    ///   - vocabOnly: Indicates wether booleans should be kept together to avoid misalignment during copy-by-value., defaults to `false`.
    ///   - useMmap: Indicates if mmap should be used., defaults to `true`.
    ///   - useMlock: Forces the system to keep model in RAM, defaults to `false`.
    public init(
        systemPrompt: String = Defaults.defaultLlama2SystemPrompt,
        maxOutputLength: Int = 1024,
        addBosToken: Bool = false,
        gpuLayerCount: Int32 = 1,
        mainGpu: Int32 = 0,
        tensorSplit: UnsafePointer<Float>? = nil,
        progressCallback: LlamaProgressCallback? = nil,
        progressCallbackUserData: UnsafeMutableRawPointer? = nil,
        vocabOnly: Bool = false,
        useMmap: Bool = true,
        useMlock: Bool = false
    ) {
        self.wrapped = llama_model_default_params()
        
        self.systemPrompt = systemPrompt
        self.maxOutputLength = maxOutputLength
        self.addBosToken = addBosToken
        
        /// Overwrite `gpuLayerCount` in case of a simulator target environment
        #if targetEnvironment(simulator)
        self.gpuLayerCount = 0     // Disable Metal on simulator as crash otherwise
        #else
        self.gpuLayerCount = gpuLayerCount
        #endif
        self.mainGpu = mainGpu
        self.tensorSplit = tensorSplit
        self.progressCallback = progressCallback
        self.progressCallbackUserData = progressCallbackUserData
        self.vocabOnly = vocabOnly
        self.useMmap = useMmap
        self.useMlock = useMlock
    }
}
